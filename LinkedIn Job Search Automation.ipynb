{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e08334",
   "metadata": {},
   "source": [
    "# ************************** LinkedIn Job Scraping ************************ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54003cb0",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "Since we are using ipywidgets and IPython.display packages specifically designed for Jupyter Notebook and other Jupyter environments, we request you to use the Jupyter Notebook for executing the code and not the .py file. The Source code has four user-defined functions in the main function. They are \n",
    "\n",
    "1. ScrapingToMongoDb()\n",
    "2. Generate_Form()\n",
    "3. Filtering()\n",
    "4. Future_Salary_Filter()\n",
    "\n",
    "Please run only the functions you are interested in. \n",
    "The first function ScrapingToMongoDb()  when run will prompt you to enter a job title you are interested in.\n",
    "The filtering () function will prompt you to input values into the form. \n",
    "\n",
    "The MongoDB database data dump is present in the compressed folder as BSON dump. \n",
    "\n",
    "There is an ADDITIONAL FILTERING section towards the end of the notebook to illustrate the functions of the filtering\n",
    "application.\n",
    "\n",
    "\n",
    "#### Please run the 4 functions and scroll down to the main function to begin execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5cbac",
   "metadata": {},
   "source": [
    "### Loading the required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2798716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import requests\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'} \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains as ac\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pyautogui # to press down arrow key\n",
    "from html.parser import HTMLParser \n",
    "import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42c2c6",
   "metadata": {},
   "source": [
    "### Enter the Job you are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9fc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapingToMongoDb():\n",
    "    user_input = input(\"Enter the Job you are interested in: \")\n",
    "\n",
    "    # Print the input string\n",
    "    print(\"You entered:\", user_input)\n",
    "\n",
    "    # Using Selenium to search and Scroll through LinkedIn jobs\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver.exe\")\n",
    "    browser.maximize_window()\n",
    "    url= \"https://www.linkedin.com/\"\n",
    "\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Open LinkedIn website and click on Jobs\n",
    "    ###################################################################################################\n",
    "\n",
    "    url = \"https://www.linkedin.com/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #print(soup)\n",
    "\n",
    "    jobs_link = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/nav/ul/li[4]/a'))\n",
    "    )\n",
    "    jobs_link.click()\n",
    "\n",
    "    search_bar = WebDriverWait(browser, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Search job titles or companies']\"))\n",
    "    )\n",
    "    search_bar.click()\n",
    "\n",
    "    search_bar.send_keys(user_input) \n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Keep scrolling and load more jobs(5 times)\n",
    "    ###################################################################################################\n",
    "    for i in range(2): # You can change this to scrape for more jobs\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "        pyautogui.press('down',presses=100)\n",
    "\n",
    "\n",
    "        ShowMoreJobs_button = WebDriverWait(browser, 20).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main-content\"]/section[2]/button'))).click()\n",
    "\n",
    "    ###################################################################################################\n",
    "    #Write page source to HTML file and download it. \n",
    "    ###################################################################################################\n",
    "\n",
    "    pageSource = browser.page_source\n",
    "\n",
    "    fileToWrite = open(\"LinkedIn_Source.html\", \"w\")\n",
    "    fileToWrite.write(pageSource)\n",
    "\n",
    "    print(\"Downloaded LinkedIn_Source.html file Successfully!\")\n",
    "    fileToWrite.close()\n",
    "\n",
    "    #Create Beautiful soup object\n",
    "    fileToRead = open(\"LinkedIn_Source.html\", \"r\")\n",
    "    soup = BeautifulSoup(fileToRead.read())\n",
    "\n",
    "    JobLinks = []\n",
    "    card_list = soup.find_all('div', {'class': 'base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card'})\n",
    "    for card in card_list:\n",
    "        link = card.find('a')['href']\n",
    "        JobLinks.append(link)\n",
    "\n",
    "    print(len(JobLinks))\n",
    "\n",
    "    # Download the HTML files\n",
    "\n",
    "    i=1 # Last index of the HTML file downloaded\n",
    "    for link in JobLinks:\n",
    "        response = requests.get(link,headers = user_agent)\n",
    "        soup1 = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        with open(\"Linkedin_Job_NEW_[%d].html\"%(i), \"w\", encoding='utf-8') as file:\n",
    "            file.write(soup1.prettify())\n",
    "            file.close()\n",
    "        i=i+1\n",
    "\n",
    "    # Parsing the content\n",
    "\n",
    "    ###################################################################################################\n",
    "    # Defining Empty Lists\n",
    "    Job_Title=[]\n",
    "    CName = []\n",
    "    CUrl = []\n",
    "    Job_Location = []\n",
    "    No_of_Applicants = []\n",
    "\n",
    "    Recruiter_Name = []\n",
    "    Recruiter_Title = []\n",
    "    Recruiter_Url = []\n",
    "\n",
    "    Job_Descriptions=[]\n",
    "    Job_Levels=[]\n",
    "    Job_Types=[]\n",
    "    Job_function =[]\n",
    "    Job_Industry =[]\n",
    "    Similar_Jobs_Links=[]\n",
    "    Date_Downloaded=[]\n",
    "    Date_Posted=[]\n",
    "    ###################################################################################################\n",
    "\n",
    "    for i in range(len(JobLinks)):\n",
    "    #for i in range(940,1038):\n",
    "        with open(\"Linkedin_Job_NEW_[%d].html\"%(i+1), 'r') as file: # Change the file name\n",
    "            html = file.read()\n",
    "            soup_html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            #TITLE\n",
    "            if (soup_html.select('#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div>h1')):\n",
    "                title = soup_html.select('#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div>h1')\n",
    "                Job_Title.append(title[0].text.strip())\n",
    "            else:\n",
    "                Job_Title.append(None)\n",
    "\n",
    "            #Company name and #Company LinkedIn URL\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span:nth-child(1) > a\")):\n",
    "                name = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span:nth-child(1) > a\")\n",
    "                CName.append(name[0].text.strip())\n",
    "                CUrl.append(name[0].get(\"href\"))\n",
    "            else:\n",
    "                CName.append(None)\n",
    "                CUrl.append(None)\n",
    "\n",
    "\n",
    "            # Job location\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span.topcard__flavor.topcard__flavor--bullet\")):        \n",
    "                loc = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(1) > span.topcard__flavor.topcard__flavor--bullet\")\n",
    "                Job_Location.append(loc[0].text.strip())\n",
    "            else:\n",
    "                 Job_Location.append(None)\n",
    "\n",
    "            # Number of Applicants\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > figure > figcaption\")):\n",
    "                num = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > figure > figcaption\")\n",
    "                No_of_Applicants.append(num[0].text.strip())\n",
    "            else:\n",
    "                No_of_Applicants.append(None)\n",
    "\n",
    "            #RECRUITER DETAILS\n",
    "\n",
    "            # Rec Name\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > img\")):\n",
    "\n",
    "                rec_name = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > img\")\n",
    "                Recruiter_Name.append(rec_name[0]['alt'])\n",
    "            else:\n",
    "                Recruiter_Name.append(None)\n",
    "\n",
    "\n",
    "            # Rec Title\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__info.self-center.ml-1.flex-1.relative.break-words.papabear\\:min-w-0.mamabear\\:min-w-0.babybear\\:w-full > h4\")):\n",
    "                rec_title = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__info.self-center.ml-1.flex-1.relative.break-words.papabear\\:min-w-0.mamabear\\:min-w-0.babybear\\:w-full > h4\")\n",
    "                Recruiter_Title.append(rec_title[0].text.strip())\n",
    "            else:\n",
    "                Recruiter_Title.append(None)\n",
    "\n",
    "            #Rec Link\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__ctas.z-\\[3\\].self-center.ml-3.babybear\\:ml-1.babybear\\:self-start > a\")):\n",
    "                rec_message_link = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.message-the-recruiter > div > div.base-main-card__ctas.z-\\[3\\].self-center.ml-3.babybear\\:ml-1.babybear\\:self-start > a\")\n",
    "                Recruiter_Url.append(rec_message_link[0].get(\"href\"))\n",
    "            else:\n",
    "                Recruiter_Url.append(None)\n",
    "\n",
    "            # JOB DESCRIPTION\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.description__text.description__text--rich > section > div\")):\n",
    "\n",
    "                job_descr = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > div.description__text.description__text--rich > section > div\")\n",
    "                Job_Descriptions.append(job_descr[0].text.strip())\n",
    "            else:\n",
    "                Job_Descriptions.append(None)\n",
    "\n",
    "            # JOB LEVEL\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(1) > span\")):\n",
    "                job_level = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(1) > span\")\n",
    "                Job_Levels.append(job_level[0].text.strip())\n",
    "\n",
    "            else:\n",
    "                Job_Levels.append(None)\n",
    "\n",
    "            # JOB TYPE\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(2) > span\")):\n",
    "                type_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(2) > span\")\n",
    "                Job_Types.append(type_job[0].text.strip())\n",
    "\n",
    "            else:\n",
    "                Job_Types.append(None)\n",
    "\n",
    "            # JOB FUNCTION\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(3) > span\")):\n",
    "                function_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(3) > span\")\n",
    "                Job_function.append(function_job[0].text.strip())\n",
    "            else:\n",
    "                Job_function.append(None)\n",
    "\n",
    "            # JOB INDUSTRY\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(4) > span\")):\n",
    "                industry_job = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > div > section.core-section-container.my-3.description > div > ul > li:nth-child(4) > span\")\n",
    "                Job_Industry.append(industry_job[0].text.strip())\n",
    "            else:\n",
    "                Job_Industry.append(None)\n",
    "\n",
    "            # SIMILAR JOB LINKS\n",
    "\n",
    "            if (soup_html.find_all('a', {'class': 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]'})):\n",
    "                similar_jobs_cards = soup_html.find_all('a', {'class': 'base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]'})\n",
    "\n",
    "                Similar_Jobs_Links_for_this_job =[]\n",
    "                for card in similar_jobs_cards:\n",
    "                    sim_link = card.get('href')\n",
    "                    Similar_Jobs_Links_for_this_job.append(sim_link)\n",
    "\n",
    "                Similar_Jobs_Links.append(Similar_Jobs_Links_for_this_job)\n",
    "            else:\n",
    "                Similar_Jobs_Links.append(None)\n",
    "\n",
    "            # DATE DOWNLOADED\n",
    "            today = str(datetime.date.today())\n",
    "            Date_Downloaded.append(today)\n",
    "\n",
    "            # DATE POSTED\n",
    "\n",
    "            if (soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > span\")):\n",
    "                posted = soup_html.select(\"#main-content > section.core-rail.mx-auto.papabear\\:w-core-rail-width.mamabear\\:max-w-\\[790px\\].babybear\\:max-w-\\[790px\\] > div > section.top-card-layout.container-lined.overflow-hidden.babybear\\:rounded-\\[0px\\] > div > div.top-card-layout__entity-info-container.flex.flex-wrap.papabear\\:flex-nowrap > div > h4 > div:nth-child(2) > span\")\n",
    "                Date_Posted.append(posted[0].text.strip())\n",
    "            else:\n",
    "                Date_Posted.append(None)\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"JOB TITLES\")\n",
    "    print(Job_Title)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"COMPANY NAMES\")\n",
    "    print(CName)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"COMPANY URLS\")\n",
    "    print(CUrl)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB LOCATIONS\")\n",
    "    print(Job_Location)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"# APPLICANTS\")\n",
    "    print(No_of_Applicants)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#DATE POSTED\")\n",
    "    print(Date_Posted)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"DATE DOWNLOADED\")\n",
    "    print(Date_Downloaded)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"RECRUITER NAME\")\n",
    "    print(Recruiter_Name)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"RECRUITER TITLE\")\n",
    "    print(Recruiter_Title)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"RECRUITER LINKEDIN URL\")\n",
    "    print(Recruiter_Url)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    print(\"JOB DESCRIPTIONS\")\n",
    "    print(Job_Descriptions)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB LEVELS\")\n",
    "    print(Job_Levels)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB TYPES\")\n",
    "    print(Job_Types)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB FUNCTIONS\")\n",
    "    print(Job_function)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"JOB INDUSTRY\")\n",
    "    print(Job_Industry)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"SIMILAR JOBS\")\n",
    "    print(Similar_Jobs_Links) \n",
    "    print(\"\\n\")   \n",
    "\n",
    "    # ################################################################################################################\n",
    "\n",
    "    # Connecting to Mongo DB\n",
    "\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "\n",
    "    # Inserting into the MongoDB\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    x=0\n",
    "    for i in range(len(JobLinks)-1):\n",
    "\n",
    "        data1 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"TITLE\": Job_Title[i],\n",
    "                    \"COMPANY\": CName[i],\n",
    "                    \"LOCATION\": Job_Location[i],\n",
    "                    \"#Applicants\": No_of_Applicants[i],\n",
    "                    \"Date Posted\": Date_Posted[i],\n",
    "                    \"Job URL\": JobLinks[i],\n",
    "                    \"Company URL\": CUrl[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i]                \n",
    "                }\n",
    "\n",
    "        data2 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"Recruiter Name\": Recruiter_Name[i],\n",
    "                    \"Recruiter Title\": Recruiter_Title[i],\n",
    "                    \"Recruiter URL\": Recruiter_Url[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "        data3 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"DESCRIPTIONS\": Job_Descriptions[i],\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "        data4 = {\n",
    "                    \"ID\": x+1,\n",
    "                    \"LEVEL\": Job_Levels[i],\n",
    "                    \"TYPE\": Job_Types[i],\n",
    "                    \"FUNCTION\": Job_function[i],\n",
    "                    \"INDUSTRY\": Job_Industry[i],\n",
    "                    \"SIMILAR JOBS\": Similar_Jobs_Links[i] ,\n",
    "                    \"Date_Downloaded\" : Date_Downloaded[i] \n",
    "                }\n",
    "\n",
    "        collection1.insert_one(data1)\n",
    "        collection2.insert_one(data2)\n",
    "        collection3.insert_one(data3)\n",
    "        collection4.insert_one(data4)\n",
    "\n",
    "        print(\"Job [%d] Details inserted into MongoDB successfully!\"%(x+1))\n",
    "        print(\"/n\")\n",
    "        x=x+1\n",
    "\n",
    "    print(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4ccbf",
   "metadata": {},
   "source": [
    "# PERSONALIZED JOB FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad37f4f",
   "metadata": {},
   "source": [
    "### ENTER TITLE/ KEYWORD/ LOCATION/ TYPE/ INDUSTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5103f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filtering(job_type1,keyword,location,job_type2,industry): \n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "\n",
    "    user_input1 = job_type1.value\n",
    "    user_input5 = keyword.value\n",
    "    user_input2 = location.value\n",
    "    user_input3 = job_type2.value\n",
    "    user_input4 = industry.value\n",
    "\n",
    "    # print(user_input1)\n",
    "    # print(user_input5)\n",
    "    # print(user_input2)\n",
    "    # print(user_input3)\n",
    "    # print(user_input4)\n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Filter collection1 based on user_input1 and user_input2\n",
    "    if user_input1:\n",
    "        filter_1 = {'TITLE': {'$regex': '.*' + user_input1 + '.*', '$options': 'i'}}\n",
    "    else:\n",
    "        filter_1 = {}\n",
    "\n",
    "    if user_input2:\n",
    "        filter_2 = {'LOCATION': {'$regex': '.*' + user_input2 + '.*'}}\n",
    "    else:\n",
    "        filter_2 = {}\n",
    "\n",
    "    filtered_docs_1 = collection1.find({'$and': [filter_1, filter_2]})\n",
    "    #print(collection1.count_documents({'$and': [filter_1, filter_2]}))\n",
    "\n",
    "    ####################################################################################################\n",
    "    # Filter collection4 based on user_input3 and user_input4\n",
    "    if user_input3:\n",
    "        filter_3 = {'TYPE': {'$regex': '.*' + user_input3 + '.*'}}\n",
    "    else:\n",
    "        filter_3 = {}\n",
    "\n",
    "    if user_input4:\n",
    "        filter_4 = {'INDUSTRY': {'$regex': '.*' + user_input4 + '.*'}}\n",
    "    else:\n",
    "        filter_4 = {}\n",
    "\n",
    "    filtered_docs_4 = collection4.find({'$and': [filter_3, filter_4]})\n",
    "    #print(collection4.count_documents({'$and': [filter_3, filter_4]}))\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Filter collection3 based on user_input5\n",
    "    if user_input5:\n",
    "        filter_5 = {'DESCRIPTIONS': {'$regex': '.*' + user_input5 + '.*'}}\n",
    "    else:\n",
    "        filter_5 = {}\n",
    "\n",
    "    filtered_docs_5 = collection3.find(filter_5)\n",
    "    #print(collection3.count_documents(filter_5))\n",
    "\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Get the IDs from filtered_docs_1\n",
    "    ids_1 = [doc['ID'] for doc in filtered_docs_1]\n",
    "\n",
    "    # Get the IDs from filtered_docs_4\n",
    "    ids_4 = [doc['ID'] for doc in filtered_docs_4]\n",
    "\n",
    "    # Get the IDs from filtered_docs_5\n",
    "    ids_5 = [doc['ID'] for doc in filtered_docs_5]\n",
    "    ####################################################################################################\n",
    "\n",
    "    # Find the common IDs\n",
    "\n",
    "    common_ids = set(ids_1).intersection(set(ids_4)).intersection(set(ids_5))\n",
    "\n",
    "    n6= collection1.count_documents({'ID': {'$in': list(common_ids)}})\n",
    "    print(\"Found %d jobs!\"%n6)\n",
    "    ####################################################################################################\n",
    "\n",
    "    JOBS6=[]\n",
    "    # Print the jobs with common IDs\n",
    "    for doc in collection1.find({'ID': {'$in': list(common_ids)}}):\n",
    "        JOBS6.append([doc['ID'], doc['TITLE'], doc['COMPANY'], doc['LOCATION'], doc['#Applicants'],doc['Date Posted'],doc['Job URL'],doc['Company URL'],doc['Date_Downloaded'] ])\n",
    "\n",
    "    if(filter_1==filter_2==filter_3==filter_4==filter_5=={}):\n",
    "        print('')\n",
    "    else:\n",
    "        df6 = pd.DataFrame(JOBS6, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "        print(df6)\n",
    "        df6.to_csv('filtered_jobs.csv', index=False)\n",
    "\n",
    "\n",
    "    ####################################################################################################\n",
    "    # DOWNLOAD THE FILTERED JOBS AS CSV\n",
    "     ####################################################################################################\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756f27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Form():\n",
    "    job_type1 = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Data Scientist/Data Analyst/Engineer/Product Manager', \n",
    "        description='What Job are you looking for?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    keyword = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='SQL/Python/Mathematics/Computer Science/AI/neural networks',\n",
    "        description='Enter the keyword you are looking for in the Job Description:',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    location = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='TX/OH/FL/CA/San Francisco/NY/United States',\n",
    "        description='Which Location?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    job_type2 = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Full-time/Internship/Contract',\n",
    "        description='What type of job are you looking for?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    industry = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Software/Technology/Health Care/Music',\n",
    "        description='Which Industry do you prefer?',\n",
    "        disabled=False,\n",
    "        layout={'width': '100%'},\n",
    "        style={'description_width': '300px'}\n",
    "    )\n",
    "\n",
    "    button = widgets.Button(description='Submit')\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        print('What Job are you looking for?', job_type1.value)\n",
    "        print('Enter the keyword you are looking for:', job_type1.value)\n",
    "        print('Which Location?', location.value)\n",
    "        print('What type of job are you looking for?', job_type2.value)\n",
    "        print('Which Industry do you prefer?', industry.value)\n",
    "        #print(f\"Filtering results for {job_type1.value} with keyword '{keyword_text}' in {location_text} for a {job_type_text} job in the {industry_text} industry.\")\n",
    "\n",
    "        \n",
    "        Filtering(job_type1,keyword,location,job_type2,industry)\n",
    "\n",
    "\n",
    "    button.on_click(on_button_clicked)\n",
    "\n",
    "    form = widgets.VBox([job_type1, keyword, location, job_type2, industry, button])\n",
    "    display(form)\n",
    "    \n",
    "    return job_type1,keyword,location,job_type2,industry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d76c8c",
   "metadata": {},
   "source": [
    "# FUTURE SCOPE: SALARY FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03219015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Future_Salary_Filter():\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "    db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "    collection1 = db[\"JOBS_Basic\"]\n",
    "    collection2 = db[\"JOBS_Recruiter\"]\n",
    "    collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "    collection4 = db[\"JOBS_Advanced\"]\n",
    "    \n",
    "    for document in collection3.find():\n",
    "        description = str(document.get(\"DESCRIPTIONS\", \"\"))\n",
    "\n",
    "        match = re.search(r\"\\$(\\d{1,3}(?:,\\d{3})*)(?:\\D+)?\\$(\\d{1,3}(?:,\\d{3})*)\", description)\n",
    "        lower_salary = match.group(1).replace(\",\", \"\") if match else None\n",
    "        upper_salary = match.group(2).replace(\",\", \"\") if match else None\n",
    "        lower_salary= float(lower_salary) if lower_salary else None\n",
    "        upper_salary= float(upper_salary) if lower_salary else None\n",
    "\n",
    "        # update the document with the new columns\n",
    "        collection3.update_one({\"_id\": document[\"_id\"]}, {\"$set\": {\"lower_salary\": lower_salary, \"upper_salary\": upper_salary}})\n",
    "\n",
    "    print(\"The current approach of using regex to add salary ranges to the Jobs_Description collection has resulted in inaccurate values because it includes both yearly and hourly pay rates. To address this issue in the future, a more precise regex function could be developed that can handle different salary formats, such as base pay, hourly pay rate, and upper and lower limits for annual pay rates. The extracted values could be converted uniformly to hourly or annual pay rates and stored in the database. This enhancement would enable the search filter to match user input values with job offerings based on their expected salary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d48450",
   "metadata": {},
   "source": [
    "# MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0261d288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ca7c70ad6549a8977be9b796dd51b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='What Job are you looking for?', layout=Layout(width='100%'), placehâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1277 jobs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try: \n",
    "        import pymongo\n",
    "        client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "        db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "        collection1 = db[\"JOBS_Basic\"]\n",
    "        collection2 = db[\"JOBS_Recruiter\"]\n",
    "        collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "        collection4 = db[\"JOBS_Advanced\"]\n",
    "        #####################################################################################################\n",
    "        #Running this will ask you to enter a job title, through a Selenium browser will open the LinkedIn page\n",
    "        #and scroll for jobs and inserts the job details in the database\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### FIRST FUNCTION CALL ###\n",
    "        #ScrapingToMongoDb()\n",
    "        \n",
    "        #####################################################################################################\n",
    "        #Generates a Personalized form asking you to input values\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### SECOND FUNCTION CALL ###\n",
    "        job_type1,keyword,location,job_type2,industry = Generate_Form()\n",
    "        \n",
    "        \n",
    "        #####################################################################################################\n",
    "        # Call filtering function with the user inputs.Filters the MongoDB database and downloads the csv file\n",
    "        #####################################################################################################\n",
    "        ### THIRD FUNCTION CALL ###\n",
    "        \n",
    "        Filtering(job_type1,keyword,location,job_type2,industry)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################################################################################################\n",
    "        #This function is for the the additional feature that can be added in the future to query the database for salary ranges\n",
    "        #####################################################################################################\n",
    "        \n",
    "        ### FOURTH FUNCTION CALL ###\n",
    "        #Future_Salary_Filter()\n",
    "        \n",
    "        #####################################################################################################\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('Error: ' + str(ex))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24bb55",
   "metadata": {},
   "source": [
    "# ADDITIONAL FILTERING ILLUSTRATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67a41c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "db = client[\"LinkedIn_DB\"]\n",
    "\n",
    "collection1 = db[\"JOBS_Basic\"]\n",
    "collection2 = db[\"JOBS_Recruiter\"]\n",
    "collection3 = db[\"JOBS_DESCRIPTION\"]\n",
    "collection4 = db[\"JOBS_Advanced\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5d47e",
   "metadata": {},
   "source": [
    "Printing all the basic Job Details from Collection 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08be49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOBS1 = []\n",
    "\n",
    "# for job in collection1.find():\n",
    "#     JOBS1.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "# df1 = pd.DataFrame(JOBS1, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "# print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541f321",
   "metadata": {},
   "source": [
    "### 1. By Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6674481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 jobs!\n",
      "      ID                                              TITLE  \\\n",
      "0      8                          Machine Learning Engineer   \n",
      "1     14                                     Data Scientist   \n",
      "2     23                                     Data Scientist   \n",
      "3     38  Data Scientist, ML (US only) - Freelance [Remote]   \n",
      "4     63                  Associate Data Scientist (Remote)   \n",
      "5     64                 Machine Learning Engineer (Remote)   \n",
      "6    112           Machine Learning Engineer - Only The USA   \n",
      "7    122                   Senior Machine Learning Engineer   \n",
      "8    188                             Data Scientist/Analyst   \n",
      "9    212                Data Science Engineer (Entry Level)   \n",
      "10   263                             Data Scientist/Analyst   \n",
      "11   284                   Senior Data Scientist I- Adtech.   \n",
      "12   316                            Data Analyst, Marketing   \n",
      "13   422                                       Data Analyst   \n",
      "14   426                                       Data Analyst   \n",
      "15   441                                       Data Analyst   \n",
      "16   463                                       Data Analyst   \n",
      "17   467                                       Data Analyst   \n",
      "18   485  Data Analyst with Tableau , PowerBI and R and ...   \n",
      "19   543                          Presentation Data Analyst   \n",
      "20   587                                Senior Data Analyst   \n",
      "21   613                            Data Analyst, Marketing   \n",
      "22   649                           Product Manager - Growth   \n",
      "23   652                  Product Manager (Multiple Levels)   \n",
      "24   657                                    Product Manager   \n",
      "25   659                          Associate Product Manager   \n",
      "26   696                                    Product Manager   \n",
      "27   713                             REMOTE Product Manager   \n",
      "28   719                           Consumer Product Manager   \n",
      "29   728   Senior Product Manager, Personalization & Search   \n",
      "30   762           Product Manager, MBA - Summer 2023 Start   \n",
      "31   768                          Associate Product Manager   \n",
      "32   773                                    Product Manager   \n",
      "33   780                                    Product Manager   \n",
      "34   784                    Product Manager I, Google Cloud   \n",
      "35   801                             Senior Product Manager   \n",
      "36   818                                    Product Manager   \n",
      "37   834                                    Product Manager   \n",
      "38   840                   Senior Product Manager, Payments   \n",
      "39   873  R-1384832 - Staff Product Manager - Social Pro...   \n",
      "40   879                                    Product Manager   \n",
      "41   880                 Associate Product Manager (Remote)   \n",
      "42   893                          Product Manager, Payments   \n",
      "43   903                Product Manager (Remote USA/Canada)   \n",
      "44   905                                    Product Manager   \n",
      "45   906                                    Product Manager   \n",
      "46   927  Lead Product Manager (First Product Management...   \n",
      "47   937           Product Manager, MBA - Summer 2023 Start   \n",
      "48   956              Data Engineer, Analytics (Generalist)   \n",
      "49   976                                      Data Engineer   \n",
      "50   978                                      Data Engineer   \n",
      "51  1006               Data Engineer - Intern (Summer 2023)   \n",
      "52  1024                Data Engineer, Database Engineering   \n",
      "53    43                  Data Analyst in San Francisco, CA   \n",
      "54    71                                       Data Analyst   \n",
      "\n",
      "                                             COMPANY                LOCATION  \\\n",
      "0                                              Yakoa       San Francisco, CA   \n",
      "1                                         Unlearn.AI  San Francisco Bay Area   \n",
      "2                                            Ingenio  San Francisco Bay Area   \n",
      "3                                         Braintrust       San Francisco, CA   \n",
      "4                                           BioSpace  San Francisco Bay Area   \n",
      "5                                       Curai Health  San Francisco Bay Area   \n",
      "6                  Manage Resources Recursos Humanos       San Francisco, CA   \n",
      "7                                              Yakoa       San Francisco, CA   \n",
      "8                                        Ursus, Inc.       San Francisco, CA   \n",
      "9                                      TEKtalent Inc  San Francisco Bay Area   \n",
      "10  Software Guidance & Assistance, Inc. (SGA, Inc.)       San Francisco, CA   \n",
      "11                                            InMobi       San Francisco, CA   \n",
      "12                             Pendulum Therapeutics       San Francisco, CA   \n",
      "13                                      NextPhase.ai       San Francisco, CA   \n",
      "14                                            NovoEd       San Francisco, CA   \n",
      "15                              USA Tech Recruitment  San Francisco Bay Area   \n",
      "16                                              SPAN       San Francisco, CA   \n",
      "17                                       Nova Credit       San Francisco, CA   \n",
      "18                      Milestone Technologies, Inc.       San Francisco, CA   \n",
      "19                           Claire Myers Consulting  San Francisco Bay Area   \n",
      "20                              LatentView Analytics  San Francisco Bay Area   \n",
      "21                             Pendulum Therapeutics       San Francisco, CA   \n",
      "22                                          Nextdoor       San Francisco, CA   \n",
      "23                                          DoorDash       San Francisco, CA   \n",
      "24                                        Boostlingo  San Francisco Bay Area   \n",
      "25                                              Tubi       San Francisco, CA   \n",
      "26                                         Dragonfly  San Francisco Bay Area   \n",
      "27                                       CyberCoders       San Francisco, CA   \n",
      "28                                          Greylock  San Francisco Bay Area   \n",
      "29                                      The RealReal       San Francisco, CA   \n",
      "30                                            Kikoff  San Francisco Bay Area   \n",
      "31                                               LHH  San Francisco Bay Area   \n",
      "32                                         OpenPhone       San Francisco, CA   \n",
      "33                                              Tome       San Francisco, CA   \n",
      "34                                            Google       San Francisco, CA   \n",
      "35                                       Forethought       San Francisco, CA   \n",
      "36                                           CURATED       San Francisco, CA   \n",
      "37                                             Adobe       San Francisco, CA   \n",
      "38                                      Maven Clinic       San Francisco, CA   \n",
      "39                               Walmart Global Tech  San Francisco Bay Area   \n",
      "40                                              Okta       San Francisco, CA   \n",
      "41                                          Antelope       San Francisco, CA   \n",
      "42                                              Loop       San Francisco, CA   \n",
      "43                                          Autodesk       San Francisco, CA   \n",
      "44                                  DeFi Recruitment  San Francisco Bay Area   \n",
      "45                                             Zynga  San Francisco Bay Area   \n",
      "46                                          Greylock  San Francisco Bay Area   \n",
      "47                                            Kikoff       San Francisco, CA   \n",
      "48                                         Instagram       San Francisco, CA   \n",
      "49                                   Stealth Startup  San Francisco Bay Area   \n",
      "50                                             Yakoa       San Francisco, CA   \n",
      "51                                        Cloudflare       San Francisco, CA   \n",
      "52                                           Experfy       San Francisco, CA   \n",
      "53                                  MATRIX Resources       San Francisco, CA   \n",
      "54                                              SPAN       San Francisco, CA   \n",
      "\n",
      "                         #Applicants  Date Posted  \\\n",
      "0                               None   1 week ago   \n",
      "1                Over 200 applicants   1 week ago   \n",
      "2                Over 200 applicants  2 weeks ago   \n",
      "3                Over 200 applicants   5 days ago   \n",
      "4                Over 200 applicants   1 week ago   \n",
      "5                Over 200 applicants   4 days ago   \n",
      "6                               None   1 week ago   \n",
      "7                               None   1 week ago   \n",
      "8                               None   1 week ago   \n",
      "9                Over 200 applicants  2 weeks ago   \n",
      "10  Be among the first 25 applicants   1 week ago   \n",
      "11  Be among the first 25 applicants  6 hours ago   \n",
      "12                              None   1 week ago   \n",
      "13                              None  2 weeks ago   \n",
      "14               Over 200 applicants   5 days ago   \n",
      "15               Over 200 applicants   1 week ago   \n",
      "16                              None   4 days ago   \n",
      "17               Over 200 applicants   4 days ago   \n",
      "18               Over 200 applicants   1 week ago   \n",
      "19               Over 200 applicants   4 days ago   \n",
      "20               Over 200 applicants   1 week ago   \n",
      "21                              None   1 week ago   \n",
      "22               Over 200 applicants   1 week ago   \n",
      "23               Over 200 applicants    1 day ago   \n",
      "24               Over 200 applicants   1 week ago   \n",
      "25               Over 200 applicants   4 days ago   \n",
      "26               Over 200 applicants   3 days ago   \n",
      "27                              None  2 weeks ago   \n",
      "28               Over 200 applicants   1 week ago   \n",
      "29                              None   1 week ago   \n",
      "30               Over 200 applicants   1 week ago   \n",
      "31               Over 200 applicants  3 weeks ago   \n",
      "32               Over 200 applicants  3 weeks ago   \n",
      "33               Over 200 applicants  2 weeks ago   \n",
      "34                              None   5 days ago   \n",
      "35               Over 200 applicants    1 day ago   \n",
      "36                              None  2 weeks ago   \n",
      "37               Over 200 applicants  2 weeks ago   \n",
      "38                              None   1 week ago   \n",
      "39               Over 200 applicants  3 weeks ago   \n",
      "40               Over 200 applicants  2 weeks ago   \n",
      "41               Over 200 applicants  2 weeks ago   \n",
      "42                              None   5 days ago   \n",
      "43                              None   1 week ago   \n",
      "44                              None  2 weeks ago   \n",
      "45                              None   1 week ago   \n",
      "46               Over 200 applicants   1 week ago   \n",
      "47                              None  2 weeks ago   \n",
      "48                              None  2 weeks ago   \n",
      "49               Over 200 applicants    1 day ago   \n",
      "50                              None   1 week ago   \n",
      "51               Over 200 applicants   1 week ago   \n",
      "52  Be among the first 25 applicants  1 month ago   \n",
      "53                              None   5 days ago   \n",
      "54                              None   1 week ago   \n",
      "\n",
      "                                              Job URL  \\\n",
      "0   https://www.linkedin.com/jobs/view/machine-lea...   \n",
      "1   https://www.linkedin.com/jobs/view/data-scient...   \n",
      "2   https://www.linkedin.com/jobs/view/data-scient...   \n",
      "3   https://www.linkedin.com/jobs/view/data-scient...   \n",
      "4   https://www.linkedin.com/jobs/view/associate-d...   \n",
      "5   https://www.linkedin.com/jobs/view/machine-lea...   \n",
      "6   https://www.linkedin.com/jobs/view/machine-lea...   \n",
      "7   https://www.linkedin.com/jobs/view/senior-mach...   \n",
      "8   https://www.linkedin.com/jobs/view/data-scient...   \n",
      "9   https://www.linkedin.com/jobs/view/data-scienc...   \n",
      "10  https://www.linkedin.com/jobs/view/data-scient...   \n",
      "11  https://www.linkedin.com/jobs/view/senior-data...   \n",
      "12  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "13  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "14  https://www.linkedin.com/jobs/view/associate-d...   \n",
      "15  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "16  https://www.linkedin.com/jobs/view/associate-d...   \n",
      "17  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "18  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "19  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "20  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "21  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "22  https://www.linkedin.com/jobs/view/associate-p...   \n",
      "23  https://www.linkedin.com/jobs/view/product-man...   \n",
      "24  https://www.linkedin.com/jobs/view/product-man...   \n",
      "25  https://www.linkedin.com/jobs/view/product-man...   \n",
      "26  https://www.linkedin.com/jobs/view/product-man...   \n",
      "27  https://www.linkedin.com/jobs/view/senior-prod...   \n",
      "28  https://www.linkedin.com/jobs/view/product-man...   \n",
      "29  https://www.linkedin.com/jobs/view/senior-prod...   \n",
      "30  https://www.linkedin.com/jobs/view/product-man...   \n",
      "31  https://www.linkedin.com/jobs/view/product-man...   \n",
      "32  https://www.linkedin.com/jobs/view/product-man...   \n",
      "33  https://www.linkedin.com/jobs/view/associate-p...   \n",
      "34  https://www.linkedin.com/jobs/view/product-man...   \n",
      "35  https://www.linkedin.com/jobs/view/product-ope...   \n",
      "36  https://www.linkedin.com/jobs/view/product-man...   \n",
      "37  https://www.linkedin.com/jobs/view/product-man...   \n",
      "38  https://www.linkedin.com/jobs/view/principal-t...   \n",
      "39  https://www.linkedin.com/jobs/view/product-man...   \n",
      "40  https://www.linkedin.com/jobs/view/product-man...   \n",
      "41  https://www.linkedin.com/jobs/view/product-man...   \n",
      "42  https://www.linkedin.com/jobs/view/senior-prod...   \n",
      "43  https://www.linkedin.com/jobs/view/product-man...   \n",
      "44  https://www.linkedin.com/jobs/view/senior-tech...   \n",
      "45  https://www.linkedin.com/jobs/view/product-man...   \n",
      "46  https://www.linkedin.com/jobs/view/technical-p...   \n",
      "47  https://www.linkedin.com/jobs/view/product-man...   \n",
      "48  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "49  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "50  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "51  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "52  https://www.linkedin.com/jobs/view/data-analyt...   \n",
      "53  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "54  https://www.linkedin.com/jobs/view/data-analys...   \n",
      "\n",
      "                                          Company URL Date_Downloaded  \n",
      "0   https://www.linkedin.com/company/yako-a?trk=pu...      2023-03-19  \n",
      "1   https://www.linkedin.com/company/unlearn-ai?tr...      2023-03-19  \n",
      "2   https://www.linkedin.com/company/ingenio?trk=p...      2023-03-19  \n",
      "3   https://www.linkedin.com/company/usebraintrust...      2023-03-19  \n",
      "4   https://www.linkedin.com/company/biospaceinc?t...      2023-03-19  \n",
      "5   https://www.linkedin.com/company/curai?trk=pub...      2023-03-19  \n",
      "6   https://ar.linkedin.com/company/manage-resourc...      2023-03-19  \n",
      "7   https://www.linkedin.com/company/yako-a?trk=pu...      2023-03-19  \n",
      "8   https://www.linkedin.com/company/ursus-incorpo...      2023-03-19  \n",
      "9   https://www.linkedin.com/company/tektalent-sta...      2023-03-19  \n",
      "10  https://www.linkedin.com/company/software-guid...      2023-03-19  \n",
      "11  https://sg.linkedin.com/company/inmobi?trk=pub...      2023-03-19  \n",
      "12  https://www.linkedin.com/company/pendulumco?tr...      2023-03-19  \n",
      "13  https://www.linkedin.com/company/nextphaseai?t...      2023-03-19  \n",
      "14  https://www.linkedin.com/company/novoed?trk=pu...      2023-03-19  \n",
      "15  https://www.linkedin.com/company/usa-recruit?t...      2023-03-19  \n",
      "16  https://www.linkedin.com/company/spanio?trk=pu...      2023-03-19  \n",
      "17  https://www.linkedin.com/company/nova-credit-i...      2023-03-19  \n",
      "18  https://www.linkedin.com/company/milestone-tec...      2023-03-19  \n",
      "19  https://www.linkedin.com/company/claire-myers-...      2023-03-19  \n",
      "20  https://in.linkedin.com/company/latentview-ana...      2023-03-19  \n",
      "21  https://www.linkedin.com/company/pendulumco?tr...      2023-03-19  \n",
      "22  https://www.linkedin.com/company/nextdoor-com?...      2023-03-19  \n",
      "23  https://www.linkedin.com/company/doordash?trk=...      2023-03-19  \n",
      "24  https://www.linkedin.com/company/boostlingo?tr...      2023-03-19  \n",
      "25  https://www.linkedin.com/company/tubi-tv?trk=p...      2023-03-19  \n",
      "26  https://www.linkedin.com/company/dragonfly-cap...      2023-03-19  \n",
      "27  https://www.linkedin.com/company/cybercoders?t...      2023-03-19  \n",
      "28  https://www.linkedin.com/company/greylock-part...      2023-03-19  \n",
      "29  https://www.linkedin.com/company/the-realreal?...      2023-03-19  \n",
      "30  https://www.linkedin.com/company/getkikoff?trk...      2023-03-19  \n",
      "31  https://www.linkedin.com/company/lee-hecht-har...      2023-03-19  \n",
      "32  https://www.linkedin.com/company/openphone?trk...      2023-03-19  \n",
      "33  https://www.linkedin.com/company/magical-tome?...      2023-03-19  \n",
      "34  https://www.linkedin.com/company/google?trk=pu...      2023-03-19  \n",
      "35  https://www.linkedin.com/company/forethought-a...      2023-03-19  \n",
      "36  https://www.linkedin.com/company/curated-com?t...      2023-03-19  \n",
      "37  https://www.linkedin.com/company/adobe?trk=pub...      2023-03-19  \n",
      "38  https://www.linkedin.com/company/mavenclinic?t...      2023-03-19  \n",
      "39  https://www.linkedin.com/company/walmartglobal...      2023-03-19  \n",
      "40  https://www.linkedin.com/company/okta-inc-?trk...      2023-03-19  \n",
      "41  https://www.linkedin.com/company/antelopepets?...      2023-03-19  \n",
      "42  https://www.linkedin.com/company/loop-payments...      2023-03-19  \n",
      "43  https://www.linkedin.com/company/autodesk?trk=...      2023-03-19  \n",
      "44  https://uk.linkedin.com/company/defi-recruitme...      2023-03-19  \n",
      "45  https://www.linkedin.com/company/zynga?trk=pub...      2023-03-19  \n",
      "46  https://www.linkedin.com/company/greylock-part...      2023-03-19  \n",
      "47  https://www.linkedin.com/company/getkikoff?trk...      2023-03-19  \n",
      "48  https://www.linkedin.com/company/instagram?trk...      2023-03-19  \n",
      "49  https://www.linkedin.com/company/stealth-start...      2023-03-19  \n",
      "50  https://www.linkedin.com/company/yako-a?trk=pu...      2023-03-19  \n",
      "51  https://www.linkedin.com/company/cloudflare?tr...      2023-03-19  \n",
      "52  https://www.linkedin.com/company/experfy?trk=p...      2023-03-19  \n",
      "53  https://www.linkedin.com/company/matrix-resour...      2023-03-23  \n",
      "54  https://www.linkedin.com/company/spanio?trk=pu...      2023-03-23  \n"
     ]
    }
   ],
   "source": [
    "# Filter jobs that contain 'San Francisco' in the 'LOCATION' field\n",
    "filtered_jobs1 = collection1.find({'LOCATION': {'$regex': '.*San Francisco.*'}})\n",
    "\n",
    "n = collection1.count_documents({'LOCATION': {'$regex': '.*San Francisco.*'}})\n",
    "JOBS1 = []\n",
    "print(\"Found %d jobs!\"%n)\n",
    "# Print the results\n",
    "for job in filtered_jobs1:\n",
    "    JOBS1.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "df2 = pd.DataFrame(JOBS1, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71cd77",
   "metadata": {},
   "source": [
    "### 2. By Skills from Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87b6ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 693 jobs!\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_DESCRIPTION collection that contain \"sql\" or \"python\" skills\n",
    "filtered_docs1 = collection3.find({'$or': [{'DESCRIPTIONS': {'$regex': '.*sql.*', '$options': 'i'}}, \n",
    "                                          {'DESCRIPTIONS': {'$regex': '.*python.*', '$options': 'i'}}\n",
    "                                          ]})\n",
    "\n",
    "N = collection3.count_documents({'$or': [{'DESCRIPTIONS': {'$regex': '.*sql.*', '$options': 'i'}}, \n",
    "                                          {'DESCRIPTIONS': {'$regex': '.*python.*', '$options': 'i'}}\n",
    "                                          ]})\n",
    "\n",
    "print(\"Found %d jobs!\"%N)\n",
    "\n",
    "ID1 = [doc['ID'] for doc in filtered_docs1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fe125",
   "metadata": {},
   "source": [
    "##### From the previous filtered skills, now filter for Data Scientist jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf86cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 233 jobs!\n",
      "      ID                                              TITLE  \\\n",
      "0      2                                     Data Scientist   \n",
      "1      3                           Data Scientist Full Time   \n",
      "2      4                                   Data Scientist I   \n",
      "3      5                                     Data Scientist   \n",
      "4      6         Junior Data Scientist / Full-time (Remote)   \n",
      "..   ...                                                ...   \n",
      "228  291        REMOTE Sr. Data Scientist (1 year contract)   \n",
      "229  292        REMOTE Sr. Data Scientist (1 year contract)   \n",
      "230  294                                     Data Scientist   \n",
      "231  296                           Associate Data Scientist   \n",
      "232  999  Data Engineer/Data Scientist/Machine Learning ...   \n",
      "\n",
      "                    COMPANY                 LOCATION  \\\n",
      "0                     Kargo            Waterford, WI   \n",
      "1         Bardess Group Ltd             New York, NY   \n",
      "2         Hebrew SeniorLife               Boston, MA   \n",
      "3          Sutoer Solutions            Fort Mill, SC   \n",
      "4                 HireMatch                Miami, FL   \n",
      "..                      ...                      ...   \n",
      "228             CyberCoders              Houston, TX   \n",
      "229             CyberCoders             New York, NY   \n",
      "230         Dezign Concepts               McLean, VA   \n",
      "231  UL Research Institutes              Raleigh, NC   \n",
      "232           WalletConnect  New York, United States   \n",
      "\n",
      "                          #Applicants  Date Posted  \\\n",
      "0                                None   1 week ago   \n",
      "1                 Over 200 applicants   1 week ago   \n",
      "2                 Over 200 applicants   1 week ago   \n",
      "3                                None  2 weeks ago   \n",
      "4                                None  2 hours ago   \n",
      "..                                ...          ...   \n",
      "228  Be among the first 25 applicants   4 days ago   \n",
      "229                              None   4 days ago   \n",
      "230  Be among the first 25 applicants   5 days ago   \n",
      "231                              None   1 week ago   \n",
      "232                              None   5 days ago   \n",
      "\n",
      "                                               Job URL  \\\n",
      "0    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "1    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "2    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "3    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "4    https://www.linkedin.com/jobs/view/junior-data...   \n",
      "..                                                 ...   \n",
      "228  https://www.linkedin.com/jobs/view/remote-sr-d...   \n",
      "229  https://www.linkedin.com/jobs/view/remote-sr-d...   \n",
      "230  https://www.linkedin.com/jobs/view/data-scient...   \n",
      "231  https://www.linkedin.com/jobs/view/associate-d...   \n",
      "232  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "\n",
      "                                           Company URL Date_Downloaded  \n",
      "0    https://www.linkedin.com/company/kargo?trk=pub...      2023-03-19  \n",
      "1    https://www.linkedin.com/company/bardess-group...      2023-03-19  \n",
      "2    https://www.linkedin.com/company/hebrew-senior...      2023-03-19  \n",
      "3    https://www.linkedin.com/company/sutoer-soluti...      2023-03-19  \n",
      "4    https://www.linkedin.com/company/hire-match?tr...      2023-03-19  \n",
      "..                                                 ...             ...  \n",
      "228  https://www.linkedin.com/company/cybercoders?t...      2023-03-19  \n",
      "229  https://www.linkedin.com/company/cybercoders?t...      2023-03-19  \n",
      "230  https://www.linkedin.com/company/dezignconcept...      2023-03-19  \n",
      "231  https://www.linkedin.com/company/ulresearchins...      2023-03-19  \n",
      "232  https://www.linkedin.com/company/walletconnect...      2023-03-19  \n",
      "\n",
      "[233 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_Basic collection that have job title \"data scientist\" and job ID from the previous filtered list\n",
    "filtered_jobs = collection1.find({'$and': [{'TITLE': {'$regex': '.*data scientist.*', '$options': 'i'}},\n",
    "                                            {'ID': {'$in': ID1}}]})\n",
    "\n",
    "N1 = collection1.count_documents({'$and': [{'TITLE': {'$regex': '.*data scientist.*', '$options': 'i'}},\n",
    "                                            {'ID': {'$in': ID1}}]})\n",
    "\n",
    "print(\"Found %d jobs!\"%N1)\n",
    "\n",
    "# Print the filtered jobs\n",
    "JOBS3=[]\n",
    "for job in filtered_jobs:\n",
    "    JOBS3.append([job['ID'], job['TITLE'], job['COMPANY'], job['LOCATION'], job['#Applicants'],job['Date Posted'],job['Job URL'],job['Company URL'],job['Date_Downloaded'] ])\n",
    "\n",
    "df3 = pd.DataFrame(JOBS3, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb7988",
   "metadata": {},
   "source": [
    "### 3. By Job Level, Type and Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aca29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 jobs!\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_ADVANCED collection that have \n",
    "#LEVEL containing \"Entry\", TYPE containing \"Full-time\", and INDUSTRY containing \"Technology\"\n",
    "\n",
    "filtered_docs2 = collection4.find({'$and': [{'LEVEL': {'$regex': '.*Entry.*', '$options': 'i'}},\n",
    "                                            {'TYPE': {'$regex': '.*Full-time.*', '$options': 'i'}},\n",
    "                                            {'INDUSTRY': {'$regex': '.*Technology.*', '$options': 'i'}}]})\n",
    "\n",
    "n2 = collection4.count_documents({'$and': [{'LEVEL': {'$regex': '.*Entry.*', '$options': 'i'}},\n",
    "                                            {'TYPE': {'$regex': '.*Full-time.*', '$options': 'i'}},\n",
    "                                            {'INDUSTRY': {'$regex': '.*Technology.*', '$options': 'i'}}]})\n",
    "print(\"Found %d jobs!\"%n2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da7092",
   "metadata": {},
   "source": [
    "##### Find Recruiter URLS for the previous filtered jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0bb98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the Recruiter profiles!\n",
      "    ID Recruiter Name                                    Recruiter Title  \\\n",
      "0    3           None                                               None   \n",
      "1   27           None                                               None   \n",
      "2   31           None                                               None   \n",
      "3   36           None                                               None   \n",
      "4   38           None                                               None   \n",
      "..  ..            ...                                                ...   \n",
      "60  36    Mary Lacore                 Connecting People to Opportunities   \n",
      "61  38           None                                               None   \n",
      "62  40           None                                               None   \n",
      "63  42           None                                               None   \n",
      "64  46   Derik Perals  I am a hardworking recent college graduate wit...   \n",
      "\n",
      "                                        Recruiter URL Date_Downloaded  \n",
      "0                                                None      2023-03-19  \n",
      "1                                                None      2023-03-19  \n",
      "2                                                None      2023-03-19  \n",
      "3                                                None      2023-03-19  \n",
      "4                                                None      2023-03-19  \n",
      "..                                                ...             ...  \n",
      "60  https://www.linkedin.com/login?session_redirec...      2023-03-19  \n",
      "61                                               None      2023-03-19  \n",
      "62                                               None      2023-03-19  \n",
      "63                                               None      2023-03-19  \n",
      "64  https://www.linkedin.com/login?session_redirec...      2023-03-19  \n",
      "\n",
      "[65 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the job IDs from the filtered documents\n",
    "\n",
    "job_ids =[]\n",
    "for doc in filtered_docs2:\n",
    "    job_ids.append(doc['ID'])\n",
    "\n",
    "#print(job_ids)\n",
    "\n",
    "# Find the recruiter URLs in the JOBS_Recruiter collection that correspond to the filtered job IDs\n",
    "filtered_recruiters = collection2.find({'ID': {'$in': job_ids}})\n",
    "\n",
    "JOBS4=[]\n",
    "\n",
    "# Print the matching recruiter URLs\n",
    "print(\"Here are the Recruiter profiles!\")\n",
    "for recruiter in filtered_recruiters:\n",
    "    #if(recruiter['Recruiter URL']):\n",
    "    JOBS4.append((recruiter['ID'],recruiter['Recruiter Name'],recruiter['Recruiter Title'],recruiter['Recruiter URL'], recruiter['Date_Downloaded']))\n",
    "\n",
    "df4 = pd.DataFrame(JOBS4, columns=['ID', 'Recruiter Name','Recruiter Title','Recruiter URL','Date_Downloaded'])\n",
    "print(df4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056010c",
   "metadata": {},
   "source": [
    "### 4. By Job Post Date and CA Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de7fbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 jobs!\n",
      "      ID                                              TITLE  \\\n",
      "0      2                                     Data Scientist   \n",
      "1      3                           Data Scientist Full Time   \n",
      "2      4                                   Data Scientist I   \n",
      "3      5                                     Data Scientist   \n",
      "4      6         Junior Data Scientist / Full-time (Remote)   \n",
      "..   ...                                                ...   \n",
      "228  291        REMOTE Sr. Data Scientist (1 year contract)   \n",
      "229  292        REMOTE Sr. Data Scientist (1 year contract)   \n",
      "230  294                                     Data Scientist   \n",
      "231  296                           Associate Data Scientist   \n",
      "232  999  Data Engineer/Data Scientist/Machine Learning ...   \n",
      "\n",
      "                    COMPANY                 LOCATION  \\\n",
      "0                     Kargo            Waterford, WI   \n",
      "1         Bardess Group Ltd             New York, NY   \n",
      "2         Hebrew SeniorLife               Boston, MA   \n",
      "3          Sutoer Solutions            Fort Mill, SC   \n",
      "4                 HireMatch                Miami, FL   \n",
      "..                      ...                      ...   \n",
      "228             CyberCoders              Houston, TX   \n",
      "229             CyberCoders             New York, NY   \n",
      "230         Dezign Concepts               McLean, VA   \n",
      "231  UL Research Institutes              Raleigh, NC   \n",
      "232           WalletConnect  New York, United States   \n",
      "\n",
      "                          #Applicants  Date Posted  \\\n",
      "0                                None   1 week ago   \n",
      "1                 Over 200 applicants   1 week ago   \n",
      "2                 Over 200 applicants   1 week ago   \n",
      "3                                None  2 weeks ago   \n",
      "4                                None  2 hours ago   \n",
      "..                                ...          ...   \n",
      "228  Be among the first 25 applicants   4 days ago   \n",
      "229                              None   4 days ago   \n",
      "230  Be among the first 25 applicants   5 days ago   \n",
      "231                              None   1 week ago   \n",
      "232                              None   5 days ago   \n",
      "\n",
      "                                               Job URL  \\\n",
      "0    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "1    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "2    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "3    https://www.linkedin.com/jobs/view/data-scient...   \n",
      "4    https://www.linkedin.com/jobs/view/junior-data...   \n",
      "..                                                 ...   \n",
      "228  https://www.linkedin.com/jobs/view/remote-sr-d...   \n",
      "229  https://www.linkedin.com/jobs/view/remote-sr-d...   \n",
      "230  https://www.linkedin.com/jobs/view/data-scient...   \n",
      "231  https://www.linkedin.com/jobs/view/associate-d...   \n",
      "232  https://www.linkedin.com/jobs/view/data-engine...   \n",
      "\n",
      "                                           Company URL Date_Downloaded  \n",
      "0    https://www.linkedin.com/company/kargo?trk=pub...      2023-03-19  \n",
      "1    https://www.linkedin.com/company/bardess-group...      2023-03-19  \n",
      "2    https://www.linkedin.com/company/hebrew-senior...      2023-03-19  \n",
      "3    https://www.linkedin.com/company/sutoer-soluti...      2023-03-19  \n",
      "4    https://www.linkedin.com/company/hire-match?tr...      2023-03-19  \n",
      "..                                                 ...             ...  \n",
      "228  https://www.linkedin.com/company/cybercoders?t...      2023-03-19  \n",
      "229  https://www.linkedin.com/company/cybercoders?t...      2023-03-19  \n",
      "230  https://www.linkedin.com/company/dezignconcept...      2023-03-19  \n",
      "231  https://www.linkedin.com/company/ulresearchins...      2023-03-19  \n",
      "232  https://www.linkedin.com/company/walletconnect...      2023-03-19  \n",
      "\n",
      "[233 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find all documents in the JOBS_Basic collection that have \"hours ago\" in the \"DATE POSTED\" field \n",
    "# and \"CA\" in the \"LOCATION\" field\n",
    "\n",
    "filtered_docs3 = collection1.find({'$and': [{'Date Posted': {'$regex': '.*hours ago.*'}},\n",
    "                                            {'LOCATION': {'$regex': '.*CA.*'}}]})\n",
    "\n",
    "n3 = collection1.count_documents({'$and': [{'Date Posted': {'$regex': '.*hours ago.*'}},\n",
    "                                            {'LOCATION': {'$regex': '.*CA.*'}}]})\n",
    "\n",
    "print(\"Found %d jobs!\"%n3)\n",
    "\n",
    "JOBS5=[]\n",
    "# Print the matching documents\n",
    "for doc in filtered_docs3:\n",
    "    JOBS5.append([doc['ID'], doc['TITLE'], doc['COMPANY'], doc['LOCATION'], doc['#Applicants'],doc['Date Posted'],doc['Job URL'],doc['Company URL'],doc['Date_Downloaded'] ])\n",
    "\n",
    "df5 = pd.DataFrame(JOBS3, columns=['ID', 'TITLE', 'COMPANY', 'LOCATION', '#Applicants', 'Date Posted', 'Job URL', 'Company URL', 'Date_Downloaded'])\n",
    "print(df5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b974e5b0282bcf0adbf7f147bc3cbccd5f91b0d65898b5cfbe5a8a858eda437"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
